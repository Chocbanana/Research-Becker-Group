{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d1711-8d44-435d-b82c-604d3f89b9e9",
   "metadata": {},
   "source": [
    "# Initial Work for Test Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdde970-c7a3-4fcf-853c-fc567b66fcb5",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb89a0-3de6-4ccd-be38-f0e118003d14",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "$\\usepackage{cancel}$\n",
    "$\\usepackage{amssymb}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632eb95f-72f6-4b11-84cd-33e48e92968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "#!pip install torch torchvision pandas numpy matplotlib scipy plotly tensorboard onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb475e0-2bfc-4d6a-aa9d-20dbb7eb0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tensorboard in the notebook\n",
    "%load_ext tensorboard\n",
    "# In order to force reload any changes done to the models package files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28295ed2-731a-4b1c-9573-9f19df5f96e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.simpleFork import Simple, Fork\n",
    "from models.danmf import DANMF\n",
    "from models.convmf import ConvMF\n",
    "from datasets.yt_vidframes import YtVidsDataset\n",
    "from framework.saveload import load_checkpoint, load_trained_model\n",
    "\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from timeit import default_timer\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd, diagsvd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5cde01-0aeb-41f8-8ae3-9bf8c09959e0",
   "metadata": {},
   "source": [
    "### Per-run user defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1961e23a-445b-4ba6-ada1-3366473f66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the machine being used\n",
    "# machine = \"Macbook\"\n",
    "machine = \"PC\"\n",
    "\n",
    "## FORMAT: {machine}, {model}, {datetime}\n",
    "state_dict_str = \"{}_{}_state-dict_{}.pt\"\n",
    "checkpoint_dict_str = \"{}_{}_checkpoint_{}.tar\"\n",
    "\n",
    "Checkpoint = namedtuple(\"Checkpoint\", [\"model\", \"epoch\", \"loss\", \"validation_loss\", \"opt_state_dict\", \"train_time\"])\n",
    "# data_dir = os.path.join(\"..\", \"data\")\n",
    "# output_dir = os.path.join(data_dir, \"output\")\n",
    "data_dir = os.path.join(\"../../\", \"data\")\n",
    "output_dir = os.path.join(data_dir, \"output\")\n",
    "tensorboard_dir = os.path.join(output_dir, \"tensorboard\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf874b-645d-4c00-83cb-3295843464c8",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "We remove the channels in the images (convert to grayscale) to more closely mimic the actual plasma dataset\n",
    "\n",
    "* [Helpful Link](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "\n",
    "#### Normalization\n",
    "* Normalize each image with respect to its unit Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82384c-93c9-4e52-9158-c1ac31218743",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ec189b-9ed7-4809-8cec-c7cc75b6b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset params\n",
    "batch_size = 25\n",
    "img_size = [54, 96]\n",
    "imgs_dir = os.path.join(data_dir, \"images_96x54\")\n",
    "\n",
    "# Load and split the data, and prep for being fed into the NN\n",
    "data = YtVidsDataset(imgs_dir)\n",
    "# Divide data into train, validation, test\n",
    "train_data, validation_data, test_data = random_split(data, [0.7, 0.2, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=(torch.cuda.is_available()), drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True, pin_memory=(torch.cuda.is_available()), drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1d863b-d541-4b73-aa75-ce19a38c8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6741 <class 'torch.Tensor'> torch.Size([54, 96])\n",
      "tensor([[0.0160, 0.0155, 0.0152,  ..., 0.0163, 0.0162, 0.0162],\n",
      "        [0.0156, 0.0158, 0.0155,  ..., 0.0160, 0.0159, 0.0158],\n",
      "        [0.0148, 0.0154, 0.0150,  ..., 0.0158, 0.0157, 0.0156],\n",
      "        ...,\n",
      "        [0.0147, 0.0146, 0.0145,  ..., 0.0140, 0.0136, 0.0141],\n",
      "        [0.0147, 0.0146, 0.0145,  ..., 0.0140, 0.0136, 0.0141],\n",
      "        [0.0146, 0.0146, 0.0145,  ..., 0.0140, 0.0136, 0.0142]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[2669])\n",
    "print(len(data), type(data[0]), data[244].shape)\n",
    "print(data[2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8fd5925-3dde-43fb-bb48-cb34a28179f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "\n",
    "### Structure\n",
    "\n",
    "#### DANMF Implementation from papers\n",
    "\n",
    "* **Non-alternating/nonnegative Matrix Factorization (NMF)**: Classical deep NMF for a non-negative matrix X ∈ Rm×n + can be described as follows: first, X is decomposed as X ≈ U0V 0 where U 0 ∈ Rm×r0 +  and V 0 ∈ Rr0×n + . The coding matrix V 0 is then further decomposed as V 0 ≈ U 1V 1 where U 1 ∈ Rr0×r1 +  and V 1 ∈ Rr1×n +. The procedure is repeated until a pre-fixed number of layers is reached. [[2]](#r2)\n",
    "* **deep alternating non-negative matrix factorisation (DA-NMF)**: we propose to factorise the basis and coding matrices in an alternating order along the layers. [[2]](#r2)\n",
    "    * Uses approx 6-8 layers\n",
    "\n",
    "<img src=\"images/r2_fig1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<!-- ![image.png](images/r2_fig1.png) -->\n",
    "\n",
    "\n",
    "### L-System\n",
    "* Using L (Lindenmayer) Systems to define strucure?\n",
    "* Node types:\n",
    "    * Terminating (1 -> 0)\n",
    "    * Splitting (1 -> 2)\n",
    "    * Unifying (2 ->1)\n",
    "    * Direct (1 -> 1)\n",
    "    * Starting (0 -> 1)\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "We use methods defined in [[1]](#r1) to define our loss function <!-- $\\mathcal{L}$ \\\\ -->\n",
    "\n",
    "$$\n",
    "\\min_{\\substack{U \\in \\real^{m\\times r} \\\\ V \\in \\real^{r\\times n}}} ||X - ReLU(UV)||^2_F ,\n",
    "$$\n",
    "Where we are finding the square Frobenius norm of the difference between the original matrix $X$ and the rectified linear low rank representation matrices $UV$\n",
    "\n",
    "#### From paper\n",
    "\n",
    "Eq. 10-11 [[2]](#r2)\n",
    "\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "Adam works fine / is standard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88ef72-8831-4488-9a7a-4211d0c43296",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebd7fd-b883-464c-82ae-e10f5ef35163",
   "metadata": {},
   "source": [
    "### General params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93616b48-16f1-4a4e-ac2f-a7c671d6dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Select device to use for compute power\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "use_cuda = device == \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e9a56-ae04-4ebd-b8f4-7807741360b9",
   "metadata": {},
   "source": [
    "### Various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8348b213-00cf-4b74-9618-6258f0294c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (seq): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=5184, out_features=500, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=400, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (U): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=324, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(54, 6))\n",
      "    )\n",
      "  )\n",
      "  (V): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=576, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(6, 96))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Fork {'stem_layer_dims': [500, 400, 300, 200], 'fork_layer_dims': [300, 200, 100, 100], 'rank': 6, 'img_size': [54, 96], 'desc': 'Linear layers that fork into two separate channels for U, V'}\n"
     ]
    }
   ],
   "source": [
    "# layer_dims = (500, 400, 300, 200, 100, 100)\n",
    "\n",
    "model_fork = Fork().to(device)\n",
    "print(model_fork)\n",
    "print(model_fork.get_name(), model_fork.get_hyperparameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7aec16-784a-46dd-80c7-3a228f055f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANMF(\n",
      "  (input): Flatten(start_dim=1, end_dim=-1)\n",
      "  (U): ModuleDict(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=5184, out_features=8100, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(54, 150))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=15000, out_features=10500, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(150, 70))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=3500, out_features=2100, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(70, 30))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1500, out_features=300, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(30, 10))\n",
      "    )\n",
      "  )\n",
      "  (V): ModuleDict(\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=14400, out_features=9600, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(100, 96))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=7000, out_features=5000, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(50, 100))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1500, out_features=500, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(10, 50))\n",
      "    )\n",
      "  )\n",
      "  (seq): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=5184, out_features=14400, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=14400, out_features=15000, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=15000, out_features=7000, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=7000, out_features=3500, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=3500, out_features=1500, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_danmf = DANMF().to(device)\n",
    "print(model_danmf)\n",
    "\n",
    "# writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model_danmf.get_name()}_{runname}'))\n",
    "# writer.add_graph(model_danmf, next(iter(train_dataloader)).to(device))\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457b75a6-8944-4e88-b88e-0bf3add65efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvMF(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (seq): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=4888, out_features=500, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (U): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=324, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(54, 6))\n",
      "    )\n",
      "  )\n",
      "  (V): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=576, bias=True)\n",
      "      (1): Unflatten(dim=1, unflattened_size=(6, 96))\n",
      "    )\n",
      "  )\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_conv = ConvMF().to(device)\n",
    "print(model_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47453944-254c-480c-8f73-e7653132b7cc",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefc746-fea1-4a25-8d25-dfa204df7cfe",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34bde206-f104-482e-9ebc-deffcb50af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(X, U, V):\n",
    "    return torch.mean(\n",
    "            torch.square(\n",
    "                torch.linalg.matrix_norm(X - nn.functional.relu(torch.bmm(U, V)),\n",
    "                # Don't enforce non-negativity on UV?\n",
    "                # torch.linalg.matrix_norm(X - torch.bmm(U, V),\n",
    "                                         ord='fro')))\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "def train_model(model, optimizer, output_run_dir, epochs=15, checkpoint_at = -1, load = True, batch_pr = 200, writer=None, profiler=None, **kwargs):\n",
    "    mname = model.get_name()\n",
    "    start_epoch = -1\n",
    "\n",
    "    print(f\"Training {mname}\")\n",
    "\n",
    "    # Attempt to load the previous checkpoint\n",
    "    if load:\n",
    "        checkpoint, statedict = load_checkpoint(mname, machine, output_run_dir)\n",
    "        if checkpoint and statedict:\n",
    "            start_epoch = checkpoint[\"epoch\"]\n",
    "            optimizer.load_state_dict(checkpoint[\"opt_state_dict\"])\n",
    "            model.load_state_dict(statedict)\n",
    "        else:\n",
    "            print(\"No checkpoint found to load. Using base model\")\n",
    "\n",
    "\n",
    "    # Save basic hyperparams\n",
    "    if writer:\n",
    "        writer.add_hparams(model.get_hyperparameters(True), {})\n",
    "        writer.flush()\n",
    "\n",
    "    loss_arr = []\n",
    "    validation_arr = []\n",
    "    time_arr = []\n",
    "    for e in range(start_epoch+1, start_epoch+1 + epochs ):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_time = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Write out a view of the NN graph, just once\n",
    "            if writer and e == 0 and i == 0:\n",
    "                writer.add_graph(model, data)\n",
    "                writer.flush()\n",
    "                torch.onnx.export(model, data, os.path.join(output_run_dir, f'{mname}_model.onnx'), input_names=[\"matrix\"], output_names=[\"V\", \"U\"])\n",
    "\n",
    "            start_time = default_timer()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            U, V = model(data)\n",
    "            # Loss function\n",
    "            loss = loss_fcn(data, U, V)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_time = default_timer() - start_time\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # Print and save statistics\n",
    "            if i % batch_pr == batch_pr - 1:    # print every 200 mini-batches\n",
    "                avg_loss = running_loss / batch_pr\n",
    "                loss_arr.append(avg_loss)\n",
    "                avg_time = running_time / batch_pr\n",
    "                time_arr.append(avg_time)\n",
    "\n",
    "                # Determine validation loss\n",
    "                model.eval()\n",
    "                model.train(False)\n",
    "                v_arr = []\n",
    "                for v_data in validation_dataloader:\n",
    "                    v_data = v_data.to(device)\n",
    "                    U_v, V_v = model(v_data)\n",
    "                    v_arr.append(loss_fcn(v_data, U_v, V_v).item())\n",
    "                validation_arr.append(np.mean(v_arr))\n",
    "                model.train(True)\n",
    "\n",
    "                # Write out stats\n",
    "                print(f\"[{e}, {i+1}] loss: {avg_loss}, validation loss: {validation_arr[-1]}, average train time (sec): {avg_time}\")\n",
    "                if writer:\n",
    "                    writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : validation_arr[-1] },\n",
    "                            e * len(train_dataloader) + i)\n",
    "                    writer.add_scalar('Average Train Time (s)', avg_time, e * len(train_dataloader) + i)\n",
    "                    writer.flush()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_time = 0.0\n",
    "\n",
    "            if profiler:\n",
    "                profiler.step()\n",
    "\n",
    "        # Save output to checkpoint dict\n",
    "        if e % checkpoint_at == checkpoint_at - 1:\n",
    "            dt = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "            save = Checkpoint(mname, e, loss_arr, validation_arr, optimizer.state_dict(), time_arr)._asdict()\n",
    "            torch.save(save, os.path.join(output_run_dir, checkpoint_dict_str.format(machine, mname, dt)))\n",
    "            torch.save(model.state_dict(), os.path.join(output_run_dir, state_dict_str.format(machine, mname, dt)))\n",
    "            print(f\"Saved checkpoint for epoch {e}: {machine}_{mname}\")\n",
    "            loss_arr = []\n",
    "            validation_arr = []\n",
    "            time_arr = []\n",
    "\n",
    "    # Always save output at end\n",
    "    dt = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "    save = Checkpoint(mname, e, loss_arr, validation_arr, optimizer.state_dict(), time_arr)._asdict()\n",
    "    torch.save(save, os.path.join(output_run_dir, checkpoint_dict_str.format(machine, mname, dt)))\n",
    "    torch.save(model.state_dict(), os.path.join(output_run_dir, state_dict_str.format(machine, mname, dt)))\n",
    "    print(f\"Saved checkpoint for epoch {e}: {machine}_{mname}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "##  torch.nn.functional.pad(input, pad, mode='constant', value=None) → Tensor\n",
    "\n",
    "def trace_handler(prof):\n",
    "    table = prof.key_averages().table(sort_by=\"self_cuda_time_total\" if use_cuda else \"self_cpu_time_total\", row_limit=10)\n",
    "    print(table)\n",
    "    # ff = prof.key_averages()\n",
    "    # df_table = pd.DataFrame(ff)\n",
    "    # print(df_table)\n",
    "    # df_table.to_csv(\"tst.csv\")\n",
    "\n",
    "    ## TODO: save to file, and tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26d659-c6b3-4dce-b0b3-11038ecb9571",
   "metadata": {},
   "source": [
    "### Multiple model running (various hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b62bf7-6b71-436d-99e7-65e019a1583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fork_r4_sdim2-3ebc_fdim2-37e7 \trank = 4 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r4_cdim2-c00f \trank = 4 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r5_sdim2-3ebc_fdim2-37e7 \trank = 5 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r5_cdim2-c00f \trank = 5 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_sdim2-3ebc_fdim2-37e7 \trank = 6 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_cdim2-c00f \trank = 6 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r7_sdim2-3ebc_fdim2-37e7 \trank = 7 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r7_cdim2-c00f \trank = 7 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r8_sdim2-3ebc_fdim2-37e7 \trank = 8 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r8_cdim2-c00f \trank = 8 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r9_sdim2-3ebc_fdim2-37e7 \trank = 9 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r9_cdim2-c00f \trank = 9 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r10_sdim2-3ebc_fdim2-37e7 \trank = 10 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r10_cdim2-c00f \trank = 10 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r11_sdim2-3ebc_fdim2-37e7 \trank = 11 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r11_cdim2-c00f \trank = 11 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n",
      "Fork_r12_sdim2-3ebc_fdim2-37e7 \trank = 12 \t sl=[500, 200] \t fl=[200, 300] \tcl=0\n",
      "ConvMF_r12_cdim2-c00f \trank = 12 \t sl=[500, 200] \t fl=[200, 300] \tcl=[[5, 1, 3, 1], [3, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "## Params\n",
    "ranks = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "stem_layer_dims = [500, 200]\n",
    "\n",
    "fork_layer_dims = [200, 300]\n",
    "\n",
    "conv_dims = [[5, 1, 3, 1], [3, 1, 0, 1]]\n",
    "\n",
    "\n",
    "# Save details\n",
    "run_details = {}\n",
    "run_details[\"run_params\"] = dict(epochs=120,\n",
    "                                 checkpoint_at=40,\n",
    "                                 batch_pr=40,\n",
    "                                 batch_size=batch_size,\n",
    "                                 subname=\"Winner1_ranks\"\n",
    "                                )\n",
    "runname = run_details[\"run_params\"][\"subname\"]\n",
    "output_run_dir = os.path.join(output_dir, f\"run_{runname}\")\n",
    "\n",
    "models = []\n",
    "\n",
    "for r in ranks:\n",
    "    m = Fork(r, img_size, stem_layer_dims, fork_layer_dims).to(device)\n",
    "    print(f\"{m.get_name()} \\trank = {r} \\t sl={stem_layer_dims} \\t fl={fork_layer_dims} \\tcl={0}\")\n",
    "    models.append(m)\n",
    "    run_details[m.get_name()] = m.get_hyperparameters()\n",
    "\n",
    "    m = ConvMF(r, img_size, stem_layer_dims, fork_layer_dims, conv_dims).to(device)\n",
    "    print(f\"{m.get_name()} \\trank = {r} \\t sl={stem_layer_dims} \\t fl={fork_layer_dims} \\tcl={conv_dims}\")\n",
    "    models.append(m)\n",
    "    run_details[m.get_name()] = m.get_hyperparameters()\n",
    "\n",
    "# m = Fork(nn_rank, img_size, stem_layer_dims[0], fork_layer_dims[0]).to(device)\n",
    "# print(f\"{m.get_name()} \\trank = {nn_rank}, \\tcl={0} \\t sl={stem_layer_dims[0]}, \\t fl={fork_layer_dims[0]}\")\n",
    "# models.append(m)\n",
    "# run_details[m.get_name()] = m.get_hyperparameters()\n",
    "# for i, cl in enumerate(conv_dims):\n",
    "#     if i == 0:\n",
    "#         for sl in stem_layer_dims:\n",
    "#             for fl in fork_layer_dims:\n",
    "#                 m = ConvMF(nn_rank, img_size, sl, fl, cl).to(device)\n",
    "#                 print(f\"{m.get_name()} \\trank = {nn_rank}, \\tcl={cl} \\t sl={sl}, \\t fl={fl}\")\n",
    "#                 models.append(m)\n",
    "#                 run_details[m.get_name()] = m.get_hyperparameters()\n",
    "#     else:\n",
    "#         m = ConvMF(nn_rank, img_size, stem_layer_dims[0], fork_layer_dims[0], cl).to(device)\n",
    "#         print(f\"{m.get_name()} \\trank = {nn_rank}, \\tcl={cl} \\t sl={stem_layer_dims[0]}, \\t fl={fork_layer_dims[0]}\")\n",
    "#         models.append(m)\n",
    "#         run_details[m.get_name()] = m.get_hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04a6361-8621-4436-8940-a78617e944e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_params': {'epochs': 120,\n",
       "  'checkpoint_at': 40,\n",
       "  'batch_pr': 40,\n",
       "  'batch_size': 25,\n",
       "  'subname': 'Winner1_ranks'},\n",
       " 'Fork_r4_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 4,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r4_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 4,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r5_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 5,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r5_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 5,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 6,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 6,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r7_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 7,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r7_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 7,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r8_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 8,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r8_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 8,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r9_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 9,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r9_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 9,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r10_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 10,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r10_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 10,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r11_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 11,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r11_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 11,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'},\n",
       " 'Fork_r12_sdim2-3ebc_fdim2-37e7': {'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 12,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Linear layers that fork into two separate channels for U, V'},\n",
       " 'ConvMF_r12_cdim2-c00f': {'conv_dims': [[5, 1, 3, 1], [3, 1, 0, 1]],\n",
       "  'stem_layer_dims': [500, 200],\n",
       "  'fork_layer_dims': [200, 300],\n",
       "  'rank': 12,\n",
       "  'img_size': [54, 96],\n",
       "  'desc': 'Conv layer(s) into Fork model'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552edf3-c542-490c-bfa5-84a22d644d90",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c89cf0db-c20b-40ec-9f82-a4aeb8a8981d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ConvMF_r12_cdim2-c00f\n",
      "Found checkpoint to load. Using: PC_ConvMF_r12_cdim2-c00f_checkpoint_2023-11-27_032136.tar\n",
      "Found model state dict to load. Using: PC_ConvMF_r12_cdim2-c00f_state-dict_2023-11-27_032136.pt\n",
      "[80, 40] loss: 0.005513838154729456, validation loss: 0.005583635891594415, average train time (sec): 0.0001989050000005932\n",
      "[80, 80] loss: 0.005673635780112818, validation loss: 0.005567064157353257, average train time (sec): 5.328750000046512e-05\n",
      "[80, 120] loss: 0.006264014943735674, validation loss: 0.007470091444633479, average train time (sec): 5.661000000003469e-05\n",
      "[80, 160] loss: 0.00872987158363685, validation loss: 0.006559977114622323, average train time (sec): 5.2627499999857716e-05\n",
      "[81, 40] loss: 0.006696912960615009, validation loss: 0.007073415188505402, average train time (sec): 4.710999999986143e-05\n",
      "[81, 80] loss: 0.006672021996928379, validation loss: 0.005752058379035795, average train time (sec): 4.2834999999286086e-05\n",
      "[81, 120] loss: 0.005777103640139103, validation loss: 0.005563334517954093, average train time (sec): 4.6467500000346716e-05\n",
      "[81, 160] loss: 0.0052797061507590115, validation loss: 0.005502663117851007, average train time (sec): 4.867749999988291e-05\n",
      "[82, 40] loss: 0.0055759783019311724, validation loss: 0.005446507107853046, average train time (sec): 4.7740000000828785e-05\n",
      "[82, 80] loss: 0.0053256315586622804, validation loss: 0.005443142234997929, average train time (sec): 4.714250000006359e-05\n",
      "[82, 120] loss: 0.005581398634240032, validation loss: 0.005460396053199217, average train time (sec): 4.7152500000891e-05\n",
      "[82, 160] loss: 0.005195353977615014, validation loss: 0.005387706304285323, average train time (sec): 4.31574999993245e-05\n",
      "[83, 40] loss: 0.005445626401342452, validation loss: 0.005420003481701297, average train time (sec): 5.416499999881808e-05\n",
      "[83, 80] loss: 0.005692870548227802, validation loss: 0.005432497529875276, average train time (sec): 4.737249999919868e-05\n",
      "[83, 120] loss: 0.005355823348509148, validation loss: 0.0053490178685916484, average train time (sec): 4.795499999943331e-05\n",
      "[83, 160] loss: 0.005322708829771728, validation loss: 0.005436568582465626, average train time (sec): 4.678250000011985e-05\n",
      "[84, 40] loss: 0.005398555332794786, validation loss: 0.005447645443228056, average train time (sec): 4.520250000012993e-05\n",
      "[84, 80] loss: 0.006030811910750345, validation loss: 0.005485683178775153, average train time (sec): 4.785999999938895e-05\n",
      "[84, 120] loss: 0.0054261989251244815, validation loss: 0.005520985250906, average train time (sec): 4.662500000023329e-05\n",
      "[84, 160] loss: 0.005094395554624498, validation loss: 0.005374339800152295, average train time (sec): 5.009999999998627e-05\n",
      "[85, 40] loss: 0.005364105524495244, validation loss: 0.005368851911592877, average train time (sec): 4.474250000043867e-05\n",
      "[85, 80] loss: 0.005369815928861499, validation loss: 0.005390818481210549, average train time (sec): 4.72500000000764e-05\n",
      "[85, 120] loss: 0.005694269493687898, validation loss: 0.005377336730301942, average train time (sec): 4.586499999987836e-05\n",
      "[85, 160] loss: 0.005288675450719893, validation loss: 0.005374448391485889, average train time (sec): 4.5885000000112085e-05\n",
      "[86, 40] loss: 0.005296719097532332, validation loss: 0.005355879453555593, average train time (sec): 4.625500000088323e-05\n",
      "[86, 80] loss: 0.005335518880747259, validation loss: 0.0053436738737630395, average train time (sec): 4.635250000006863e-05\n",
      "[86, 120] loss: 0.005357010423904285, validation loss: 0.005362652756168314, average train time (sec): 5.041500000118049e-05\n",
      "[86, 160] loss: 0.005365429114317521, validation loss: 0.005324796984358778, average train time (sec): 4.987749999969537e-05\n",
      "[87, 40] loss: 0.005507553456118331, validation loss: 0.005338232380004143, average train time (sec): 5.367750000004889e-05\n",
      "[87, 80] loss: 0.005249375751009211, validation loss: 0.005334567269837519, average train time (sec): 6.536000000068043e-05\n",
      "[87, 120] loss: 0.005036852182820439, validation loss: 0.005284158341621734, average train time (sec): 4.551000000105887e-05\n",
      "[87, 160] loss: 0.0053589975752402095, validation loss: 0.005296695630399967, average train time (sec): 4.8537499999667943e-05\n",
      "[88, 40] loss: 0.00558882457553409, validation loss: 0.005998174558868105, average train time (sec): 5.050249999953849e-05\n",
      "[88, 80] loss: 0.0057027889066375795, validation loss: 0.005461501361767076, average train time (sec): 4.53250000006733e-05\n",
      "[88, 120] loss: 0.005452385806711391, validation loss: 0.005380227577538704, average train time (sec): 5.0752500000328384e-05\n",
      "[88, 160] loss: 0.005421828688122332, validation loss: 0.005274958289140519, average train time (sec): 5.018749999976535e-05\n",
      "[89, 40] loss: 0.005140982364537194, validation loss: 0.005290985467650418, average train time (sec): 5.570499999976164e-05\n",
      "[89, 80] loss: 0.005357919726520777, validation loss: 0.005238662798941697, average train time (sec): 4.674500000021453e-05\n",
      "[89, 120] loss: 0.0051249545183964075, validation loss: 0.00522713403147206, average train time (sec): 4.8742500000287235e-05\n",
      "[89, 160] loss: 0.005379729019477963, validation loss: 0.005287422991867336, average train time (sec): 4.8920000000407524e-05\n",
      "[90, 40] loss: 0.005042434425558895, validation loss: 0.005240300090386058, average train time (sec): 5.244250000089323e-05\n",
      "[90, 80] loss: 0.005381016386672854, validation loss: 0.005243940401892617, average train time (sec): 5.0324999999418194e-05\n",
      "[90, 120] loss: 0.005859064904507249, validation loss: 0.005508463829755783, average train time (sec): 4.530500000043958e-05\n",
      "[90, 160] loss: 0.005248687241692096, validation loss: 0.005329322100634563, average train time (sec): 4.92024999999785e-05\n",
      "[91, 40] loss: 0.006216440221760422, validation loss: 0.005395654452753798, average train time (sec): 4.689750000039794e-05\n",
      "[91, 80] loss: 0.0052447913913056254, validation loss: 0.005286728009968152, average train time (sec): 4.793499999919959e-05\n",
      "[91, 120] loss: 0.005658242356730625, validation loss: 0.005390999043571218, average train time (sec): 4.500000000007276e-05\n",
      "[91, 160] loss: 0.0059669153997674584, validation loss: 0.005297671911253963, average train time (sec): 5.026249999957599e-05\n",
      "[92, 40] loss: 0.005109316803282127, validation loss: 0.005237817491914304, average train time (sec): 4.5262499999410013e-05\n",
      "[92, 80] loss: 0.004952150542521849, validation loss: 0.005172676701133825, average train time (sec): 4.5740000000193956e-05\n",
      "[92, 120] loss: 0.005427213927032426, validation loss: 0.0051680276969904605, average train time (sec): 4.73124999999186e-05\n",
      "[92, 160] loss: 0.005190006399061531, validation loss: 0.005188334390591338, average train time (sec): 4.7759999999641424e-05\n",
      "[93, 40] loss: 0.005330901953857392, validation loss: 0.005213851798852941, average train time (sec): 4.590749999948685e-05\n",
      "[93, 80] loss: 0.005392050585942343, validation loss: 0.005197552230455122, average train time (sec): 4.9735000000339366e-05\n",
      "[93, 120] loss: 0.005054064298747107, validation loss: 0.005161236466418179, average train time (sec): 4.88274999995042e-05\n",
      "[93, 160] loss: 0.004977232060628012, validation loss: 0.005191016669894727, average train time (sec): 4.998499999970818e-05\n",
      "[94, 40] loss: 0.005225425271783024, validation loss: 0.0051658999939220695, average train time (sec): 4.9270000000944944e-05\n",
      "[94, 80] loss: 0.005014766886597499, validation loss: 0.005141108112304278, average train time (sec): 4.7374999999760804e-05\n",
      "[94, 120] loss: 0.005331444443436339, validation loss: 0.0051930673104131, average train time (sec): 7.639250000011089e-05\n",
      "[94, 160] loss: 0.005420866678468883, validation loss: 0.005179029054728881, average train time (sec): 5.1842499999565916e-05\n",
      "[95, 40] loss: 0.005134498671395704, validation loss: 0.005179376776312601, average train time (sec): 5.608750000050122e-05\n",
      "[95, 80] loss: 0.0051743168500252065, validation loss: 0.00517110569613441, average train time (sec): 5.0472499999898446e-05\n",
      "[95, 120] loss: 0.00534461751813069, validation loss: 0.00519800621945903, average train time (sec): 4.197499999918364e-05\n",
      "[95, 160] loss: 0.005126259074313566, validation loss: 0.005338805196103903, average train time (sec): 4.547999999999774e-05\n",
      "[96, 40] loss: 0.005350021604681387, validation loss: 0.0053200679128321835, average train time (sec): 4.4149999999376634e-05\n",
      "[96, 80] loss: 0.005216390447458252, validation loss: 0.005340468656834004, average train time (sec): 4.652500000048576e-05\n",
      "[96, 120] loss: 0.005131677148165181, validation loss: 0.005195329110753143, average train time (sec): 5.0832499999842186e-05\n",
      "[96, 160] loss: 0.005293044477002695, validation loss: 0.005134801340799006, average train time (sec): 5.414000000030228e-05\n",
      "[97, 40] loss: 0.005170085083227605, validation loss: 0.005109933244486181, average train time (sec): 4.628249999996115e-05\n",
      "[97, 80] loss: 0.004864346102112904, validation loss: 0.0051543679492512965, average train time (sec): 4.623250000008738e-05\n",
      "[97, 120] loss: 0.00533967690425925, validation loss: 0.005135926479508854, average train time (sec): 4.971500000010565e-05\n",
      "[97, 160] loss: 0.005138896236894652, validation loss: 0.005146414331459212, average train time (sec): 4.695749999967802e-05\n",
      "[98, 40] loss: 0.005193345859879628, validation loss: 0.005150045118874537, average train time (sec): 4.77224999997361e-05\n",
      "[98, 80] loss: 0.00499838549294509, validation loss: 0.005113645223900676, average train time (sec): 4.6869999999898936e-05\n",
      "[98, 120] loss: 0.005261537397745997, validation loss: 0.005130190200189936, average train time (sec): 6.704500000012103e-05\n",
      "[98, 160] loss: 0.0051460827060509475, validation loss: 0.005181479219452671, average train time (sec): 4.771999999917398e-05\n",
      "[99, 40] loss: 0.00517780571244657, validation loss: 0.0051000028725643205, average train time (sec): 4.878999999959888e-05\n",
      "[99, 80] loss: 0.005162504100007936, validation loss: 0.005115279791665808, average train time (sec): 4.81949999993958e-05\n",
      "[99, 120] loss: 0.005800225946586579, validation loss: 0.005508055963184474, average train time (sec): 4.7612500000582256e-05\n",
      "[99, 160] loss: 0.005834001902258024, validation loss: 0.0059636288919960555, average train time (sec): 4.846250000127839e-05\n",
      "[100, 40] loss: 0.005472173588350416, validation loss: 0.005067124620150283, average train time (sec): 4.771250000032978e-05\n",
      "[100, 80] loss: 0.005377934209536761, validation loss: 0.005292063289023233, average train time (sec): 5.606499999970538e-05\n",
      "[100, 120] loss: 0.005952030664775521, validation loss: 0.005183094611637435, average train time (sec): 4.674749999935557e-05\n",
      "[100, 160] loss: 0.005025306879542768, validation loss: 0.005018919921603124, average train time (sec): 4.3670000000872736e-05\n",
      "[101, 40] loss: 0.004755662335082888, validation loss: 0.005044275903547148, average train time (sec): 4.7127499999533026e-05\n",
      "[101, 80] loss: 0.0053089532069861885, validation loss: 0.005043646636998879, average train time (sec): 5.659250000036309e-05\n",
      "[101, 120] loss: 0.0050390758842695504, validation loss: 0.004996778922296077, average train time (sec): 5.5629999999950995e-05\n",
      "[101, 160] loss: 0.005034708604216576, validation loss: 0.0050309513923975655, average train time (sec): 4.5347500000048056e-05\n",
      "[102, 40] loss: 0.005015663633821532, validation loss: 0.005020439646751532, average train time (sec): 4.5092499999555005e-05\n",
      "[102, 80] loss: 0.004791888420004398, validation loss: 0.005005947208770041, average train time (sec): 4.399000000034903e-05\n",
      "[102, 120] loss: 0.005160263826837763, validation loss: 0.004988129760296839, average train time (sec): 4.272249999957012e-05\n",
      "[102, 160] loss: 0.005049771943595261, validation loss: 0.005014751730311029, average train time (sec): 6.179250000002413e-05\n",
      "[103, 40] loss: 0.00509975262102671, validation loss: 0.005007602902621312, average train time (sec): 4.395750000014686e-05\n",
      "[103, 80] loss: 0.004593296226812526, validation loss: 0.005010592061499099, average train time (sec): 4.9704999999278246e-05\n",
      "[103, 120] loss: 0.0050078428292181345, validation loss: 0.004994158510048434, average train time (sec): 4.3252499999368865e-05\n",
      "[103, 160] loss: 0.005124892015010119, validation loss: 0.004985983628463352, average train time (sec): 4.8967499999719166e-05\n",
      "[104, 40] loss: 0.00490573161514476, validation loss: 0.0049750920877141775, average train time (sec): 4.689250000069478e-05\n",
      "[104, 80] loss: 0.004880739492364228, validation loss: 0.00498211804232648, average train time (sec): 4.533500000007962e-05\n",
      "[104, 120] loss: 0.005017519620014355, validation loss: 0.0050261501653365934, average train time (sec): 6.01150000008488e-05\n",
      "[104, 160] loss: 0.00517711088177748, validation loss: 0.005016423955138, average train time (sec): 4.748249999977361e-05\n",
      "[105, 40] loss: 0.005102561955573038, validation loss: 0.00508817024353259, average train time (sec): 5.654750000019249e-05\n",
      "[105, 80] loss: 0.0049022950930520896, validation loss: 0.0049925992520139465, average train time (sec): 4.594250000025113e-05\n",
      "[105, 120] loss: 0.005105001881020144, validation loss: 0.004998069260937144, average train time (sec): 4.562499999991587e-05\n",
      "[105, 160] loss: 0.005333690089173615, validation loss: 0.005032134795877731, average train time (sec): 4.895749999889176e-05\n",
      "[106, 40] loss: 0.0049260186962783335, validation loss: 0.004973831973126474, average train time (sec): 4.80975000002104e-05\n",
      "[106, 80] loss: 0.0050819477823097255, validation loss: 0.004945518150222751, average train time (sec): 6.007749999952239e-05\n",
      "[106, 120] loss: 0.0050309317244682464, validation loss: 0.005062091726598874, average train time (sec): 4.8119999999585164e-05\n",
      "[106, 160] loss: 0.0050710434967186305, validation loss: 0.005123298334063224, average train time (sec): 5.5612500000279395e-05\n",
      "[107, 40] loss: 0.004835594852920622, validation loss: 0.004999816777922635, average train time (sec): 5.6717500000047495e-05\n",
      "[107, 80] loss: 0.005025292548816651, validation loss: 0.005008867647463702, average train time (sec): 4.8690000001272436e-05\n",
      "[107, 120] loss: 0.004961345106130466, validation loss: 0.004956611172946275, average train time (sec): 4.692500000089694e-05\n",
      "[107, 160] loss: 0.004960682481760159, validation loss: 0.004935917031582234, average train time (sec): 4.7197499999640516e-05\n",
      "[108, 40] loss: 0.004946029838174581, validation loss: 0.004909128152747762, average train time (sec): 5.0557499999115405e-05\n",
      "[108, 80] loss: 0.004914684413233772, validation loss: 0.004910146786813748, average train time (sec): 4.767249999986234e-05\n",
      "[108, 120] loss: 0.004919213487301022, validation loss: 0.004893476666250038, average train time (sec): 4.80699999997114e-05\n",
      "[108, 160] loss: 0.004760311252903193, validation loss: 0.004915958929385217, average train time (sec): 4.8050000000898765e-05\n",
      "[109, 40] loss: 0.004716555768391117, validation loss: 0.004887468796574844, average train time (sec): 5.1779999999723714e-05\n",
      "[109, 80] loss: 0.004731922957580537, validation loss: 0.004897704769699079, average train time (sec): 4.796249999969859e-05\n",
      "[109, 120] loss: 0.004691010975511745, validation loss: 0.004868480820595374, average train time (sec): 4.836000000096874e-05\n",
      "[109, 160] loss: 0.0053045897802803665, validation loss: 0.004894956098518001, average train time (sec): 4.7630000000253855e-05\n",
      "[110, 40] loss: 0.004853948298841715, validation loss: 0.004940617073959899, average train time (sec): 4.7022500000082344e-05\n",
      "[110, 80] loss: 0.004960095201386139, validation loss: 0.0049605894403286135, average train time (sec): 4.5577500000604235e-05\n",
      "[110, 120] loss: 0.005342559743439779, validation loss: 0.004937024295048893, average train time (sec): 4.9222500000212224e-05\n",
      "[110, 160] loss: 0.004765559558290988, validation loss: 0.004903138492186114, average train time (sec): 4.708750000048667e-05\n",
      "[111, 40] loss: 0.004804198443889618, validation loss: 0.004899559898372248, average train time (sec): 4.8085000000241965e-05\n",
      "[111, 80] loss: 0.0048600885842461135, validation loss: 0.004871378178304096, average train time (sec): 4.9052499998936125e-05\n",
      "[111, 120] loss: 0.004778171511134133, validation loss: 0.0049011363199789005, average train time (sec): 4.459750000052054e-05\n",
      "[111, 160] loss: 0.004934899345971644, validation loss: 0.004908757385323351, average train time (sec): 5.554749999987507e-05\n",
      "[112, 40] loss: 0.004672944254707545, validation loss: 0.0048546342240681625, average train time (sec): 4.706249999912871e-05\n",
      "[112, 80] loss: 0.004764409502968192, validation loss: 0.004942993347902062, average train time (sec): 4.34699999999566e-05\n",
      "[112, 120] loss: 0.005088805448031053, validation loss: 0.004932443237916197, average train time (sec): 5.387499999898182e-05\n",
      "[112, 160] loss: 0.005321994866244495, validation loss: 0.004911831158371466, average train time (sec): 4.802750000010292e-05\n",
      "[113, 40] loss: 0.005211758543737233, validation loss: 0.004898021094290153, average train time (sec): 0.00040852999999998476\n",
      "[113, 80] loss: 0.005471621797187254, validation loss: 0.004890372603253092, average train time (sec): 4.6494999999424635e-05\n",
      "[113, 120] loss: 0.004821472120238468, validation loss: 0.0048897483623323015, average train time (sec): 4.596499999962589e-05\n",
      "[113, 160] loss: 0.004746704845456406, validation loss: 0.004908296442151351, average train time (sec): 4.499249999980748e-05\n",
      "[114, 40] loss: 0.004827796493191272, validation loss: 0.0048345996787385, average train time (sec): 4.926750000038283e-05\n",
      "[114, 80] loss: 0.004943023761734366, validation loss: 0.00500702426575546, average train time (sec): 5.448750000027758e-05\n",
      "[114, 120] loss: 0.004848034330643713, validation loss: 0.004939522651322889, average train time (sec): 4.552999999987151e-05\n",
      "[114, 160] loss: 0.00490013561793603, validation loss: 0.0048447528080839035, average train time (sec): 4.629249999936747e-05\n",
      "[115, 40] loss: 0.004684302560053766, validation loss: 0.004788833977830298, average train time (sec): 4.635000000092759e-05\n",
      "[115, 80] loss: 0.004817623650887981, validation loss: 0.004772187175475202, average train time (sec): 5.1917500000797646e-05\n",
      "[115, 120] loss: 0.004559623898239806, validation loss: 0.004787629421905808, average train time (sec): 4.43175000000906e-05\n",
      "[115, 160] loss: 0.004739026649622247, validation loss: 0.004794465645901718, average train time (sec): 9.340000000008785e-05\n",
      "[116, 40] loss: 0.0048447668843436984, validation loss: 0.004796931304845889, average train time (sec): 4.52975000001743e-05\n",
      "[116, 80] loss: 0.004882988182362169, validation loss: 0.004844335940192049, average train time (sec): 4.817000000088001e-05\n",
      "[116, 120] loss: 0.004697081027552485, validation loss: 0.0047542479609684, average train time (sec): 4.7220000000436356e-05\n",
      "[116, 160] loss: 0.004847009747754783, validation loss: 0.004800011711370833, average train time (sec): 4.7382500000026084e-05\n",
      "[117, 40] loss: 0.004585684364428744, validation loss: 0.004759926704760149, average train time (sec): 4.5987500000421734e-05\n",
      "[117, 80] loss: 0.004760262893978506, validation loss: 0.004736094102966336, average train time (sec): 4.800250000016604e-05\n",
      "[117, 120] loss: 0.0047852962161414325, validation loss: 0.00474053759233288, average train time (sec): 5.177500000002056e-05\n",
      "[117, 160] loss: 0.0047693885222543034, validation loss: 0.004739681052325188, average train time (sec): 5.193000000076608e-05\n",
      "[118, 40] loss: 0.004638425423763692, validation loss: 0.00476926152664676, average train time (sec): 4.9237500000742784e-05\n",
      "[118, 80] loss: 0.004834921105066314, validation loss: 0.004764526770538035, average train time (sec): 4.69199999997727e-05\n",
      "[118, 120] loss: 0.004906911455327645, validation loss: 0.004837184538186158, average train time (sec): 5.349500000022545e-05\n",
      "[118, 160] loss: 0.0048938531777821485, validation loss: 0.004908294860660186, average train time (sec): 4.761999999942645e-05\n",
      "[119, 40] loss: 0.005097324989037588, validation loss: 0.004983073170736151, average train time (sec): 5.616250000031187e-05\n",
      "[119, 80] loss: 0.004913688619853929, validation loss: 0.004801566391867006, average train time (sec): 4.919250000057218e-05\n",
      "[119, 120] loss: 0.005194256355753169, validation loss: 0.0049003494534430635, average train time (sec): 4.644249999898875e-05\n",
      "[119, 160] loss: 0.005010954127646983, validation loss: 0.004963859937698493, average train time (sec): 5.2455000000861675e-05\n",
      "Saved checkpoint for epoch 119: PC_ConvMF_r12_cdim2-c00f\n",
      "[120, 40] loss: 0.004930452647386119, validation loss: 0.0049323240418057395, average train time (sec): 4.5269999999675294e-05\n",
      "[120, 80] loss: 0.004802559415111318, validation loss: 0.0047841134815002385, average train time (sec): 5.5152500000588137e-05\n",
      "[120, 120] loss: 0.004898069961927831, validation loss: 0.004797262899612762, average train time (sec): 4.605499999996709e-05\n",
      "[120, 160] loss: 0.005684720014687628, validation loss: 0.005049195799554857, average train time (sec): 4.49674999998706e-05\n",
      "[121, 40] loss: 0.005316688399761915, validation loss: 0.004878135639646987, average train time (sec): 4.570500000085076e-05\n",
      "[121, 80] loss: 0.0047466333664488046, validation loss: 0.004761597449135668, average train time (sec): 4.7007499999551784e-05\n",
      "[121, 120] loss: 0.004423972079530358, validation loss: 0.004745637444462979, average train time (sec): 4.722749999928055e-05\n",
      "[121, 160] loss: 0.004657371033681556, validation loss: 0.004664362561976853, average train time (sec): 4.6509999999955196e-05\n",
      "[122, 40] loss: 0.004609488969435915, validation loss: 0.0046903365862749095, average train time (sec): 4.61725000008073e-05\n",
      "[122, 80] loss: 0.004475015087518841, validation loss: 0.004716373360627946, average train time (sec): 4.531750000040802e-05\n",
      "[122, 120] loss: 0.004597788443788886, validation loss: 0.004734230530887561, average train time (sec): 4.308500000007598e-05\n",
      "[122, 160] loss: 0.0048959915526211265, validation loss: 0.0046988482991198325, average train time (sec): 5.6322500000760556e-05\n",
      "[123, 40] loss: 0.004736986133502796, validation loss: 0.004673202952616058, average train time (sec): 4.847249999926362e-05\n",
      "[123, 80] loss: 0.0048070177144836634, validation loss: 0.004705218866221466, average train time (sec): 4.62599999991653e-05\n",
      "[123, 120] loss: 0.004594311688560992, validation loss: 0.004682941182147501, average train time (sec): 4.669249999977865e-05\n",
      "[123, 160] loss: 0.0044938076636753975, validation loss: 0.004697431493901981, average train time (sec): 4.637750000000551e-05\n",
      "[124, 40] loss: 0.004638598946621641, validation loss: 0.004699868668433068, average train time (sec): 5.0077500000611507e-05\n",
      "[124, 80] loss: 0.004530076269293204, validation loss: 0.004730257693291554, average train time (sec): 4.818000000028633e-05\n",
      "[124, 120] loss: 0.004739807167788967, validation loss: 0.004685590030484885, average train time (sec): 4.7100000000455114e-05\n",
      "[124, 160] loss: 0.004727269150316715, validation loss: 0.0046680777175527694, average train time (sec): 4.39249999999447e-05\n",
      "[125, 40] loss: 0.004267931240610779, validation loss: 0.00471036990594892, average train time (sec): 4.818999999969265e-05\n",
      "[125, 80] loss: 0.004715876432601363, validation loss: 0.004730696680094555, average train time (sec): 4.844249999962358e-05\n",
      "[125, 120] loss: 0.004741805198136717, validation loss: 0.0046588200197186114, average train time (sec): 5.7502500000339295e-05\n",
      "[125, 160] loss: 0.004797296784818173, validation loss: 0.004683382101884147, average train time (sec): 4.6757500000182973e-05\n",
      "[126, 40] loss: 0.004244889732217416, validation loss: 0.004706116977481628, average train time (sec): 4.569750000058548e-05\n",
      "[126, 80] loss: 0.0044482725148554895, validation loss: 0.0047035703813340865, average train time (sec): 4.614750000087042e-05\n",
      "[126, 120] loss: 0.004901950672501698, validation loss: 0.004693689378294742, average train time (sec): 4.652249999992364e-05\n",
      "[126, 160] loss: 0.004878059262409806, validation loss: 0.004631436311305975, average train time (sec): 4.228000000097154e-05\n",
      "[127, 40] loss: 0.004799761017784477, validation loss: 0.004634660519308077, average train time (sec): 4.569500000002335e-05\n",
      "[127, 80] loss: 0.004554962634574622, validation loss: 0.004689447247897679, average train time (sec): 4.2960000000391575e-05\n",
      "[127, 120] loss: 0.004857122036628425, validation loss: 0.004924128983029217, average train time (sec): 4.797249999910491e-05\n",
      "[127, 160] loss: 0.004639234632486477, validation loss: 0.004724578861639185, average train time (sec): 4.482749999965563e-05\n",
      "[128, 40] loss: 0.004721425240859389, validation loss: 0.004601551268144317, average train time (sec): 4.282999999958292e-05\n",
      "[128, 80] loss: 0.004357489908579737, validation loss: 0.004608779021029202, average train time (sec): 4.380249999940134e-05\n",
      "[128, 120] loss: 0.004631349327974021, validation loss: 0.004667080218358984, average train time (sec): 4.6067499999935536e-05\n",
      "[128, 160] loss: 0.004779145587235689, validation loss: 0.004632319741056494, average train time (sec): 4.442999999980657e-05\n",
      "[129, 40] loss: 0.004600884177489206, validation loss: 0.004633155581102056, average train time (sec): 5.9377499999868634e-05\n",
      "[129, 80] loss: 0.0047464091447182, validation loss: 0.004735201961835319, average train time (sec): 4.2977500000063175e-05\n",
      "[129, 120] loss: 0.004702981881564483, validation loss: 0.00469551885605983, average train time (sec): 5.147999999906005e-05\n",
      "[129, 160] loss: 0.004696346336277202, validation loss: 0.004619979036023032, average train time (sec): 5.154499999946438e-05\n",
      "[130, 40] loss: 0.004563697800040245, validation loss: 0.004651313788204823, average train time (sec): 5.315750000107755e-05\n",
      "[130, 80] loss: 0.004543270531576127, validation loss: 0.004638012085672257, average train time (sec): 5.540750000108119e-05\n",
      "[130, 120] loss: 0.0045961554511450235, validation loss: 0.004652797090733108, average train time (sec): 4.7267499999748e-05\n",
      "[130, 160] loss: 0.004776009672787041, validation loss: 0.004588760672804882, average train time (sec): 5.416499999881808e-05\n",
      "[131, 40] loss: 0.004756904015084729, validation loss: 0.004626386179500875, average train time (sec): 4.689250000069478e-05\n",
      "[131, 80] loss: 0.004478296957677231, validation loss: 0.004606709991283012, average train time (sec): 4.5307499999580617e-05\n",
      "[131, 120] loss: 0.00450159625033848, validation loss: 0.004619421678999404, average train time (sec): 4.3604999999047325e-05\n",
      "[131, 160] loss: 0.004458018019795418, validation loss: 0.004595519148937936, average train time (sec): 4.481250000054615e-05\n",
      "[132, 40] loss: 0.004418838577112183, validation loss: 0.004744756958043238, average train time (sec): 4.2764999999178596e-05\n",
      "[132, 80] loss: 0.004666220443323254, validation loss: 0.004615723133473745, average train time (sec): 4.824499999926957e-05\n",
      "[132, 120] loss: 0.004835492349229753, validation loss: 0.0046717029082465845, average train time (sec): 5.049250000013217e-05\n",
      "[132, 160] loss: 0.004362869693432003, validation loss: 0.00458635914332743, average train time (sec): 4.9344999999334506e-05\n",
      "[133, 40] loss: 0.00433412793208845, validation loss: 0.004573123204469119, average train time (sec): 5.5494999999439186e-05\n",
      "[133, 80] loss: 0.00453273524180986, validation loss: 0.004567216088960193, average train time (sec): 5.1442499999154737e-05\n",
      "[133, 120] loss: 0.004688068048562854, validation loss: 0.004586981468886699, average train time (sec): 4.71450000020468e-05\n",
      "[133, 160] loss: 0.004670920653734356, validation loss: 0.004573552368454776, average train time (sec): 5.03524999999172e-05\n",
      "[134, 40] loss: 0.004639309225603938, validation loss: 0.004600250711313115, average train time (sec): 5.045999999993e-05\n",
      "[134, 80] loss: 0.0045512538403272625, validation loss: 0.004584712428831548, average train time (sec): 5.4877499999861355e-05\n",
      "[134, 120] loss: 0.004511410108534619, validation loss: 0.004599761389160775, average train time (sec): 4.849500000148055e-05\n",
      "[134, 160] loss: 0.004547141469083726, validation loss: 0.004620737839876762, average train time (sec): 4.943499999967571e-05\n",
      "[135, 40] loss: 0.004565319191897288, validation loss: 0.004609408942139092, average train time (sec): 4.545750000204407e-05\n",
      "[135, 80] loss: 0.004870055435458198, validation loss: 0.004599027181887683, average train time (sec): 4.699750000156655e-05\n",
      "[135, 120] loss: 0.0045944211597088724, validation loss: 0.004600531250273563, average train time (sec): 4.470249999997122e-05\n",
      "[135, 160] loss: 0.004563141718972474, validation loss: 0.00453685589117121, average train time (sec): 4.5792500000629846e-05\n",
      "[136, 40] loss: 0.004872043844079599, validation loss: 0.004563931173662532, average train time (sec): 4.8792500001582086e-05\n",
      "[136, 80] loss: 0.0045440105197485535, validation loss: 0.0047818440416792654, average train time (sec): 9.194749999892337e-05\n",
      "[136, 120] loss: 0.004828714171890169, validation loss: 0.004584213832990741, average train time (sec): 4.3714999998201164e-05\n",
      "[136, 160] loss: 0.004365970002254471, validation loss: 0.004565758617454261, average train time (sec): 4.8560000001884875e-05\n",
      "[137, 40] loss: 0.00445917954784818, validation loss: 0.004549400349376055, average train time (sec): 4.92024999999785e-05\n",
      "[137, 80] loss: 0.004286682413658127, validation loss: 0.004526264310972589, average train time (sec): 4.5192499999302524e-05\n",
      "[137, 120] loss: 0.00444836167152971, validation loss: 0.004495066190841344, average train time (sec): 4.700249999984862e-05\n",
      "[137, 160] loss: 0.004756211186759174, validation loss: 0.004507028251745791, average train time (sec): 4.7589999999786416e-05\n",
      "[138, 40] loss: 0.0043721938971430065, validation loss: 0.004535203263655586, average train time (sec): 9.754249999787135e-05\n",
      "[138, 80] loss: 0.00449813422455918, validation loss: 0.004486991228746637, average train time (sec): 4.600750000065546e-05\n",
      "[138, 120] loss: 0.0045844777952879666, validation loss: 0.004791528105138327, average train time (sec): 4.6604999999999565e-05\n",
      "[138, 160] loss: 0.0045782032771967355, validation loss: 0.004620722486233374, average train time (sec): 4.896250000001601e-05\n",
      "[139, 40] loss: 0.004262229654705152, validation loss: 0.004476350617928887, average train time (sec): 4.687250000188214e-05\n",
      "[139, 80] loss: 0.00498951852787286, validation loss: 0.004569304100993388, average train time (sec): 5.120250000061333e-05\n",
      "[139, 120] loss: 0.004398778104223311, validation loss: 0.004498676388120314, average train time (sec): 4.5610000000806394e-05\n",
      "[139, 160] loss: 0.004642945277737454, validation loss: 0.004745574944810766, average train time (sec): 4.423249999945256e-05\n",
      "[140, 40] loss: 0.004964360018493608, validation loss: 0.004764823853653276, average train time (sec): 4.517750000161413e-05\n",
      "[140, 80] loss: 0.004593100515194237, validation loss: 0.004570756054852369, average train time (sec): 4.542500000184191e-05\n",
      "[140, 120] loss: 0.005206902627833188, validation loss: 0.0054870300732974735, average train time (sec): 5.4142499999443315e-05\n",
      "[140, 160] loss: 0.004908526735380292, validation loss: 0.004633516477385782, average train time (sec): 4.604249999999866e-05\n",
      "[141, 40] loss: 0.004506444721482694, validation loss: 0.004515099690629626, average train time (sec): 4.626749999943058e-05\n",
      "[141, 80] loss: 0.004301759513327852, validation loss: 0.004695698421202459, average train time (sec): 4.104250000125376e-05\n",
      "[141, 120] loss: 0.004658215440576896, validation loss: 0.00452584407801898, average train time (sec): 4.558249999888631e-05\n",
      "[141, 160] loss: 0.005073641217313707, validation loss: 0.005052915009898397, average train time (sec): 4.7892500001012193e-05\n",
      "[142, 40] loss: 0.004844292689813301, validation loss: 0.004761672684184785, average train time (sec): 4.7094999999330867e-05\n",
      "[142, 80] loss: 0.004598470259224996, validation loss: 0.004666613744360939, average train time (sec): 4.7842500001138434e-05\n",
      "[142, 120] loss: 0.004778887995053082, validation loss: 0.004599928583528074, average train time (sec): 4.750249999858624e-05\n",
      "[142, 160] loss: 0.005119822727283463, validation loss: 0.004539755621116678, average train time (sec): 5.1357500001358855e-05\n",
      "[143, 40] loss: 0.004547165113035589, validation loss: 0.0044918431030621505, average train time (sec): 5.398250000041571e-05\n",
      "[143, 80] loss: 0.004391523753292858, validation loss: 0.004489302059705809, average train time (sec): 4.787000000021635e-05\n",
      "[143, 120] loss: 0.004492981574730948, validation loss: 0.004450210954396511, average train time (sec): 5.3345000000604156e-05\n",
      "[143, 160] loss: 0.004396123206242919, validation loss: 0.0044573174156951455, average train time (sec): 4.687250000188214e-05\n",
      "[144, 40] loss: 0.004215697885956615, validation loss: 0.004478113141508316, average train time (sec): 8.19575000008399e-05\n",
      "[144, 80] loss: 0.0043319121876265855, validation loss: 0.004447745616143604, average train time (sec): 4.547249999973246e-05\n",
      "[144, 120] loss: 0.004613321804208681, validation loss: 0.004454751244201412, average train time (sec): 4.672499999855972e-05\n",
      "[144, 160] loss: 0.004346136894309893, validation loss: 0.0044541751113632375, average train time (sec): 4.67849999978398e-05\n",
      "[145, 40] loss: 0.0041545911983121185, validation loss: 0.004419861374563485, average train time (sec): 4.670749999888812e-05\n",
      "[145, 80] loss: 0.004533445229753852, validation loss: 0.004445559929458881, average train time (sec): 4.9377499999536666e-05\n",
      "[145, 120] loss: 0.004603359272005036, validation loss: 0.004459962134583379, average train time (sec): 5.186750000234497e-05\n",
      "[145, 160] loss: 0.004331294150324539, validation loss: 0.004456980184669483, average train time (sec): 4.546250000032614e-05\n",
      "[146, 40] loss: 0.004313278483459726, validation loss: 0.004456743619949469, average train time (sec): 4.7632499999394895e-05\n",
      "[146, 80] loss: 0.004052173317177221, validation loss: 0.0044362671093417785, average train time (sec): 4.394750000074055e-05\n",
      "[146, 120] loss: 0.004754935659002512, validation loss: 0.004428870352160818, average train time (sec): 4.152999999860185e-05\n",
      "[146, 160] loss: 0.004487615230027586, validation loss: 0.0044322155222718445, average train time (sec): 4.2882500000018806e-05\n",
      "[147, 40] loss: 0.004375167924445122, validation loss: 0.0044069520399888165, average train time (sec): 4.290000000253258e-05\n",
      "[147, 80] loss: 0.004340747720561922, validation loss: 0.004451085123637656, average train time (sec): 4.4449999998619204e-05\n",
      "[147, 120] loss: 0.00434987444896251, validation loss: 0.004453553712733512, average train time (sec): 4.308000000037282e-05\n",
      "[147, 160] loss: 0.0041332687484100464, validation loss: 0.004453229968313058, average train time (sec): 5.34899999991012e-05\n",
      "[148, 40] loss: 0.004302837967406959, validation loss: 0.004443043144419789, average train time (sec): 6.11199999980272e-05\n",
      "[148, 80] loss: 0.0047146558703389015, validation loss: 0.0047683097193685344, average train time (sec): 4.394999999988158e-05\n",
      "[148, 120] loss: 0.004474511265289039, validation loss: 0.0045488634419117895, average train time (sec): 4.815499999892836e-05\n",
      "[148, 160] loss: 0.0045097868307493625, validation loss: 0.004476275800217716, average train time (sec): 4.997249999973974e-05\n",
      "[149, 40] loss: 0.004487859120126813, validation loss: 0.004470935429639692, average train time (sec): 4.979249999905733e-05\n",
      "[149, 80] loss: 0.004195913078729063, validation loss: 0.004386370522760839, average train time (sec): 6.115500000021256e-05\n",
      "[149, 120] loss: 0.004594217473641038, validation loss: 0.0044096992614696615, average train time (sec): 4.5107500000085565e-05\n",
      "[149, 160] loss: 0.004506414767820388, validation loss: 0.0044172220898546135, average train time (sec): 5.174750000094264e-05\n",
      "[150, 40] loss: 0.00443108276813291, validation loss: 0.004395194733866824, average train time (sec): 4.797999999937019e-05\n",
      "[150, 80] loss: 0.004576549958437681, validation loss: 0.004414816013590063, average train time (sec): 4.850500000088687e-05\n",
      "[150, 120] loss: 0.004340002703247592, validation loss: 0.004426013287332542, average train time (sec): 4.7115000000985674e-05\n",
      "[150, 160] loss: 0.004264504660386592, validation loss: 0.004433231076823091, average train time (sec): 4.5912499999190004e-05\n",
      "[151, 40] loss: 0.004508096148492768, validation loss: 0.004457628767375114, average train time (sec): 4.618499999935466e-05\n",
      "[151, 80] loss: 0.004514651937643066, validation loss: 0.00439680970432061, average train time (sec): 5.0102500000548386e-05\n",
      "[151, 120] loss: 0.004292223497759551, validation loss: 0.0044365481403217, average train time (sec): 5.050750000066273e-05\n",
      "[151, 160] loss: 0.004160839621908963, validation loss: 0.004468986320854077, average train time (sec): 4.8567499999307986e-05\n",
      "[152, 40] loss: 0.0043349249812308695, validation loss: 0.004446669916604769, average train time (sec): 4.5324999999252216e-05\n",
      "[152, 80] loss: 0.004425346507923677, validation loss: 0.00441339576862893, average train time (sec): 4.734249999955864e-05\n",
      "[152, 120] loss: 0.004449817433487624, validation loss: 0.004409285908002617, average train time (sec): 4.558249999888631e-05\n",
      "[152, 160] loss: 0.004280838865088299, validation loss: 0.0044282504427686055, average train time (sec): 4.418750000070304e-05\n",
      "[153, 40] loss: 0.004491250327555462, validation loss: 0.004434675132012311, average train time (sec): 4.879249999873991e-05\n",
      "[153, 80] loss: 0.004461570963030681, validation loss: 0.0044016345131720576, average train time (sec): 5.431250000071941e-05\n",
      "[153, 120] loss: 0.004136668128194287, validation loss: 0.0043752358057799765, average train time (sec): 5.752749999885509e-05\n",
      "[153, 160] loss: 0.004461202508537099, validation loss: 0.004334041172251949, average train time (sec): 5.2132499999402174e-05\n",
      "[154, 40] loss: 0.004204779997235164, validation loss: 0.004380649228070704, average train time (sec): 4.229250000093998e-05\n",
      "[154, 80] loss: 0.004681343532865867, validation loss: 0.00435515407490421, average train time (sec): 4.270750000046064e-05\n",
      "[154, 120] loss: 0.003958096983842552, validation loss: 0.00436644574170405, average train time (sec): 7.161249999967367e-05\n",
      "[154, 160] loss: 0.004286009591305628, validation loss: 0.004386197924965395, average train time (sec): 4.698000000189495e-05\n",
      "[155, 40] loss: 0.004314401501324028, validation loss: 0.004413723497528513, average train time (sec): 4.9322500001380834e-05\n",
      "[155, 80] loss: 0.004339349153451621, validation loss: 0.004349139655219778, average train time (sec): 4.839750000087406e-05\n",
      "[155, 120] loss: 0.004324672248912975, validation loss: 0.004414005459831009, average train time (sec): 4.607250000105978e-05\n",
      "[155, 160] loss: 0.0045131304010283205, validation loss: 0.004397067698245903, average train time (sec): 4.630249999877378e-05\n",
      "[156, 40] loss: 0.004374844551784917, validation loss: 0.00438887311391673, average train time (sec): 4.477500000064083e-05\n",
      "[156, 80] loss: 0.0042176582908723505, validation loss: 0.004382422971451339, average train time (sec): 4.7139999998080385e-05\n",
      "[156, 120] loss: 0.0044962306914385405, validation loss: 0.0044983314780764425, average train time (sec): 4.735999999923024e-05\n",
      "[156, 160] loss: 0.004629576957086101, validation loss: 0.00475309026989875, average train time (sec): 4.630999999903907e-05\n",
      "[157, 40] loss: 0.004685905884252861, validation loss: 0.004552183057761418, average train time (sec): 5.188250000003336e-05\n",
      "[157, 80] loss: 0.0050313645275309685, validation loss: 0.004469383723225515, average train time (sec): 5.255750000117132e-05\n",
      "[157, 120] loss: 0.004233484447468072, validation loss: 0.004431457320263363, average train time (sec): 4.880500000012944e-05\n",
      "[157, 160] loss: 0.004632486577611417, validation loss: 0.004443865761442005, average train time (sec): 4.656499999953212e-05\n",
      "[158, 40] loss: 0.0042954865552019324, validation loss: 0.004339728649389351, average train time (sec): 4.569500000002335e-05\n",
      "[158, 80] loss: 0.00436403242056258, validation loss: 0.004338970574778768, average train time (sec): 5.0537500001723856e-05\n",
      "[158, 120] loss: 0.004315505444537848, validation loss: 0.004318101603959529, average train time (sec): 5.498499999987416e-05\n",
      "[158, 160] loss: 0.004485479375580326, validation loss: 0.004354315260776652, average train time (sec): 5.675250000081178e-05\n",
      "[159, 40] loss: 0.004221530596259982, validation loss: 0.0042906989044738265, average train time (sec): 4.769749999979922e-05\n",
      "[159, 80] loss: 0.004032196127809584, validation loss: 0.004365317092962422, average train time (sec): 4.697500000077071e-05\n",
      "[159, 120] loss: 0.004506351827876643, validation loss: 0.004342053766485374, average train time (sec): 5.2707499997950434e-05\n",
      "[159, 160] loss: 0.004406870895763859, validation loss: 0.00436488803880254, average train time (sec): 4.969999999957508e-05\n",
      "Saved checkpoint for epoch 159: PC_ConvMF_r12_cdim2-c00f\n",
      "[160, 40] loss: 0.004343240975867957, validation loss: 0.00433958422849482, average train time (sec): 4.660999999828164e-05\n",
      "[160, 80] loss: 0.004308323311852292, validation loss: 0.004376364419377356, average train time (sec): 5.7812500000409275e-05\n",
      "[160, 120] loss: 0.004586875147651881, validation loss: 0.004336678813967221, average train time (sec): 4.675000000133878e-05\n",
      "[160, 160] loss: 0.0041282344667706635, validation loss: 0.004275700160882102, average train time (sec): 5.029250000063712e-05\n",
      "[161, 40] loss: 0.0043858831049874425, validation loss: 0.004343103766792787, average train time (sec): 4.9172499998917374e-05\n",
      "[161, 80] loss: 0.004265256063081324, validation loss: 0.004318014283682096, average train time (sec): 4.6159999999417775e-05\n",
      "[161, 120] loss: 0.00438911541714333, validation loss: 0.004331803678552497, average train time (sec): 5.226499999935186e-05\n",
      "[161, 160] loss: 0.004182657133787871, validation loss: 0.004284675782194959, average train time (sec): 4.674500000021453e-05\n",
      "[162, 40] loss: 0.0043016448442358525, validation loss: 0.004368249379660723, average train time (sec): 4.6827500000290456e-05\n",
      "[162, 80] loss: 0.004367455933243036, validation loss: 0.004362627099496576, average train time (sec): 4.653000000018892e-05\n",
      "[162, 120] loss: 0.0042821563023608174, validation loss: 0.004387095785822789, average train time (sec): 4.555499999980839e-05\n",
      "[162, 160] loss: 0.004147571168141439, validation loss: 0.004400332383635752, average train time (sec): 4.780250000067099e-05\n",
      "[163, 40] loss: 0.004434019525069744, validation loss: 0.004372463240903223, average train time (sec): 4.7564999999849536e-05\n",
      "[163, 80] loss: 0.004328093532240018, validation loss: 0.004339996193764063, average train time (sec): 4.7832499998889946e-05\n",
      "[163, 120] loss: 0.0043153783888556065, validation loss: 0.00438518805938931, average train time (sec): 4.5860000000175205e-05\n",
      "[163, 160] loss: 0.0042156692186836155, validation loss: 0.004300869026063186, average train time (sec): 5.102000000078988e-05\n",
      "[164, 40] loss: 0.0043841528473421935, validation loss: 0.004378330190929602, average train time (sec): 4.6189999997636734e-05\n",
      "[164, 80] loss: 0.0047549071023240685, validation loss: 0.004995753650360231, average train time (sec): 4.2134999998211245e-05\n",
      "[164, 120] loss: 0.004711093130754307, validation loss: 0.0046432740582188345, average train time (sec): 4.699499999958334e-05\n",
      "[164, 160] loss: 0.004500315524637699, validation loss: 0.0043254359229907115, average train time (sec): 4.65200000007826e-05\n",
      "[165, 40] loss: 0.00424333456903696, validation loss: 0.004369847291975089, average train time (sec): 4.4900000000325234e-05\n",
      "[165, 80] loss: 0.0045765779505018145, validation loss: 0.004373359164433941, average train time (sec): 4.921000000024378e-05\n",
      "[165, 120] loss: 0.004238350782543421, validation loss: 0.00431994997175797, average train time (sec): 4.560749999882319e-05\n",
      "[165, 160] loss: 0.0043738556210882965, validation loss: 0.004291451162711348, average train time (sec): 4.705000000058135e-05\n",
      "[166, 40] loss: 0.004293050628621131, validation loss: 0.00429132159465467, average train time (sec): 4.956249999850115e-05\n",
      "[166, 80] loss: 0.003988031606422737, validation loss: 0.004297643579226339, average train time (sec): 4.8382500000343495e-05\n",
      "[166, 120] loss: 0.004600921913515776, validation loss: 0.004270891593066308, average train time (sec): 5.0095000000283106e-05\n",
      "[166, 160] loss: 0.0041313462657853964, validation loss: 0.004283382886809841, average train time (sec): 5.6089999998221177e-05\n",
      "[167, 40] loss: 0.0039783189888112245, validation loss: 0.004276691588907028, average train time (sec): 4.922749999991538e-05\n",
      "[167, 80] loss: 0.00424240185529925, validation loss: 0.0042727464317994296, average train time (sec): 4.973750000090149e-05\n",
      "[167, 120] loss: 0.004393226245883852, validation loss: 0.00430614226673431, average train time (sec): 4.644750000011299e-05\n",
      "[167, 160] loss: 0.004276088252663612, validation loss: 0.004287734104953003, average train time (sec): 4.461250000247219e-05\n",
      "[168, 40] loss: 0.004116556933149695, validation loss: 0.004265400137365708, average train time (sec): 5.068750000134514e-05\n",
      "[168, 80] loss: 0.003965647896984592, validation loss: 0.004298009755933341, average train time (sec): 4.803999999865027e-05\n",
      "[168, 120] loss: 0.004320941719925031, validation loss: 0.004335912089478576, average train time (sec): 4.725500000120064e-05\n",
      "[168, 160] loss: 0.0045266399451065805, validation loss: 0.00427224517817486, average train time (sec): 5.011000000081367e-05\n",
      "[169, 40] loss: 0.004449669871246442, validation loss: 0.00434463371013133, average train time (sec): 4.760250000117594e-05\n",
      "[169, 80] loss: 0.0043531962961424146, validation loss: 0.004402319900691509, average train time (sec): 4.225250000047254e-05\n",
      "[169, 120] loss: 0.004149357724236325, validation loss: 0.004313621582147086, average train time (sec): 4.7567500001832744e-05\n",
      "[169, 160] loss: 0.004084814537782222, validation loss: 0.004281632395742356, average train time (sec): 5.168750000166256e-05\n",
      "[170, 40] loss: 0.004223218682454899, validation loss: 0.004276466336241871, average train time (sec): 4.526749999911317e-05\n",
      "[170, 80] loss: 0.004280117654707283, validation loss: 0.004250078984834957, average train time (sec): 5.123500000081549e-05\n",
      "[170, 120] loss: 0.004264639056054875, validation loss: 0.004402692117816153, average train time (sec): 4.605500000138818e-05\n",
      "[170, 160] loss: 0.004152201220858842, validation loss: 0.004269861194463271, average train time (sec): 4.648250000229837e-05\n",
      "[171, 40] loss: 0.004315077018691227, validation loss: 0.004235334457161854, average train time (sec): 4.915750000122898e-05\n",
      "[171, 80] loss: 0.004171097348444164, validation loss: 0.004262935080266786, average train time (sec): 4.4850000000451475e-05\n",
      "[171, 120] loss: 0.004246372275520116, validation loss: 0.004290537052032239, average train time (sec): 4.8584999998979586e-05\n",
      "[171, 160] loss: 0.004093209170969203, validation loss: 0.004272342879184574, average train time (sec): 5.275750000066637e-05\n",
      "[172, 40] loss: 0.004315900907386094, validation loss: 0.00431123522619875, average train time (sec): 4.853499999910582e-05\n",
      "[172, 80] loss: 0.0039016470254864544, validation loss: 0.004254688188117349, average train time (sec): 4.683249999857253e-05\n",
      "[172, 120] loss: 0.004260134242940694, validation loss: 0.004239743399732518, average train time (sec): 4.5469999997749254e-05\n",
      "[172, 160] loss: 0.00435854812967591, validation loss: 0.004266760500921112, average train time (sec): 4.523499999891101e-05\n",
      "[173, 40] loss: 0.0041514896380249414, validation loss: 0.004290581140491479, average train time (sec): 4.724000000067008e-05\n",
      "[173, 80] loss: 0.004067730263341218, validation loss: 0.004232987027861319, average train time (sec): 4.778999999928146e-05\n",
      "[173, 120] loss: 0.00448804561747238, validation loss: 0.004271551435110423, average train time (sec): 5.1644999999211905e-05\n",
      "[173, 160] loss: 0.004391837929142639, validation loss: 0.004391698966259664, average train time (sec): 4.7362500001213445e-05\n",
      "[174, 40] loss: 0.004306843981612474, validation loss: 0.004332056545810598, average train time (sec): 4.70074999981307e-05\n",
      "[174, 80] loss: 0.004154979140730574, validation loss: 0.004287673788637204, average train time (sec): 4.80175000006966e-05\n",
      "[174, 120] loss: 0.0042475111433304845, validation loss: 0.004294870438862522, average train time (sec): 4.675499999962085e-05\n",
      "[174, 160] loss: 0.0045758322812616825, validation loss: 0.004355372320684903, average train time (sec): 4.895250000060969e-05\n",
      "[175, 40] loss: 0.008263835514662787, validation loss: 0.0073792120213356785, average train time (sec): 4.3407499998693313e-05\n",
      "[175, 80] loss: 0.013132198713719845, validation loss: 0.010551987628821494, average train time (sec): 4.469000000142387e-05\n",
      "[175, 120] loss: 0.008468920446466655, validation loss: 0.00546215421449885, average train time (sec): 4.3714999998201164e-05\n",
      "[175, 160] loss: 0.004914386861491949, validation loss: 0.0047726087526963005, average train time (sec): 5.3350000001728404e-05\n",
      "[176, 40] loss: 0.004847518127644434, validation loss: 0.004555702903571556, average train time (sec): 5.445500000007541e-05\n",
      "[176, 80] loss: 0.004619630565866828, validation loss: 0.004538835303084749, average train time (sec): 4.922749999991538e-05\n",
      "[176, 120] loss: 0.004364645015448332, validation loss: 0.004487394118013809, average train time (sec): 5.0070000000346226e-05\n",
      "[176, 160] loss: 0.004332240344956518, validation loss: 0.0044628132512476645, average train time (sec): 4.899749999935921e-05\n",
      "[177, 40] loss: 0.0041837339289486405, validation loss: 0.004401070834576803, average train time (sec): 4.842250000081094e-05\n",
      "[177, 80] loss: 0.004354556312318891, validation loss: 0.004383797291666269, average train time (sec): 4.971250000096461e-05\n",
      "[177, 120] loss: 0.004098684334894642, validation loss: 0.004400685741478261, average train time (sec): 4.897500000140553e-05\n",
      "[177, 160] loss: 0.0045255791861563924, validation loss: 0.004371073268318795, average train time (sec): 5.003750000014407e-05\n",
      "[178, 40] loss: 0.004196023906115442, validation loss: 0.004359004918029005, average train time (sec): 5.8884999998554124e-05\n",
      "[178, 80] loss: 0.004245645797345788, validation loss: 0.004338044106503422, average train time (sec): 4.419750000010936e-05\n",
      "[178, 120] loss: 0.004428322671446949, validation loss: 0.0043446038594856014, average train time (sec): 4.75100000016937e-05\n",
      "[178, 160] loss: 0.004245296871522442, validation loss: 0.004306996570689217, average train time (sec): 5.303249999997206e-05\n",
      "[179, 40] loss: 0.004338526865467429, validation loss: 0.00428608004530927, average train time (sec): 4.6352499998647546e-05\n",
      "[179, 80] loss: 0.004268407740164548, validation loss: 0.004333403541372632, average train time (sec): 4.992500000184918e-05\n",
      "[179, 120] loss: 0.004190906503936276, validation loss: 0.00430358313726929, average train time (sec): 4.6060000002512425e-05\n",
      "[179, 160] loss: 0.004144549235934392, validation loss: 0.004280936148650241, average train time (sec): 5.002750000073775e-05\n",
      "[180, 40] loss: 0.004229090380249545, validation loss: 0.00430134139550885, average train time (sec): 4.475249999984499e-05\n",
      "[180, 80] loss: 0.004235051455907524, validation loss: 0.004278142325896418, average train time (sec): 5.2395000000160505e-05\n",
      "[180, 120] loss: 0.004132549307541922, validation loss: 0.00426744374464143, average train time (sec): 4.54050000001871e-05\n",
      "[180, 160] loss: 0.004234278545482084, validation loss: 0.004279213531364528, average train time (sec): 4.446750000113298e-05\n",
      "[181, 40] loss: 0.004141388967400417, validation loss: 0.004287397748140513, average train time (sec): 4.415999999878295e-05\n",
      "[181, 80] loss: 0.004145073256222531, validation loss: 0.004274582182812803, average train time (sec): 4.572250000194345e-05\n",
      "[181, 120] loss: 0.004534602648345753, validation loss: 0.004268776322275681, average train time (sec): 4.717749999940679e-05\n",
      "[181, 160] loss: 0.004314608499407768, validation loss: 0.004268967669527486, average train time (sec): 5.170249999935095e-05\n",
      "[182, 40] loss: 0.0041052949498407544, validation loss: 0.004281309095018034, average train time (sec): 4.671000000087133e-05\n",
      "[182, 80] loss: 0.004483556078048423, validation loss: 0.004249129382858299, average train time (sec): 4.4932499997685225e-05\n",
      "[182, 120] loss: 0.003985035425284877, validation loss: 0.0042203548144689705, average train time (sec): 4.332249999947635e-05\n",
      "[182, 160] loss: 0.004262752737849951, validation loss: 0.004258942105016619, average train time (sec): 4.8050000000898765e-05\n",
      "[183, 40] loss: 0.00408529814449139, validation loss: 0.004278485751095807, average train time (sec): 4.924749999872802e-05\n",
      "[183, 80] loss: 0.004126115661347285, validation loss: 0.004217004188213427, average train time (sec): 4.4379999999932805e-05\n",
      "[183, 120] loss: 0.004338032769737765, validation loss: 0.0042481616991659945, average train time (sec): 4.723749999868687e-05\n",
      "[183, 160] loss: 0.00424548463197425, validation loss: 0.004250652793759726, average train time (sec): 4.8684999998727105e-05\n",
      "[184, 40] loss: 0.003938327950891107, validation loss: 0.004267972235058276, average train time (sec): 4.6802500000353577e-05\n",
      "[184, 80] loss: 0.004207060241606086, validation loss: 0.004255761115653616, average train time (sec): 4.4132499999705034e-05\n",
      "[184, 120] loss: 0.004706152545986697, validation loss: 0.004244380296682412, average train time (sec): 4.964500000141925e-05\n",
      "[184, 160] loss: 0.004281512269517407, validation loss: 0.004257528098159522, average train time (sec): 4.9319999999397626e-05\n",
      "[185, 40] loss: 0.004328131600050256, validation loss: 0.004224548437017596, average train time (sec): 4.436000000112017e-05\n",
      "[185, 80] loss: 0.004245552152860909, validation loss: 0.0042264643828121, average train time (sec): 4.600750000065546e-05\n",
      "[185, 120] loss: 0.004112435935530812, validation loss: 0.004219137118988723, average train time (sec): 4.661750000138909e-05\n",
      "[185, 160] loss: 0.0038818340573925523, validation loss: 0.004228220321238041, average train time (sec): 4.476750000037555e-05\n",
      "[186, 40] loss: 0.0041248926485422995, validation loss: 0.004235736858802584, average train time (sec): 4.495249999934003e-05\n",
      "[186, 80] loss: 0.003937654179753736, validation loss: 0.004215159014148533, average train time (sec): 4.328499999814994e-05\n",
      "[186, 120] loss: 0.004427452792879194, validation loss: 0.004193193797584412, average train time (sec): 4.780250000067099e-05\n",
      "[186, 160] loss: 0.004088970232987777, validation loss: 0.004223790379979138, average train time (sec): 4.551750000132415e-05\n",
      "[187, 40] loss: 0.004236713779391721, validation loss: 0.004233843923704523, average train time (sec): 4.707500000051823e-05\n",
      "[187, 80] loss: 0.004016916733235121, validation loss: 0.004215690500612529, average train time (sec): 4.8587500000962794e-05\n",
      "[187, 120] loss: 0.004039001581259072, validation loss: 0.0042073661681124065, average train time (sec): 4.6360000001754995e-05\n",
      "[187, 160] loss: 0.004202343494398519, validation loss: 0.004222987123044595, average train time (sec): 5.292750000194246e-05\n",
      "[188, 40] loss: 0.004220287944190204, validation loss: 0.0042208743167443654, average train time (sec): 4.4857500000716755e-05\n",
      "[188, 80] loss: 0.004053525818744674, validation loss: 0.004220990604668294, average train time (sec): 4.528249999964373e-05\n",
      "[188, 120] loss: 0.004255113308317959, validation loss: 0.004205873372244104, average train time (sec): 4.7354999998106e-05\n",
      "[188, 160] loss: 0.004148148943204433, validation loss: 0.004210895261252826, average train time (sec): 4.6852500000227336e-05\n",
      "[189, 40] loss: 0.004072635341435671, validation loss: 0.0042089089215293805, average train time (sec): 5.208499999866945e-05\n",
      "[189, 80] loss: 0.004290760430740193, validation loss: 0.004232911691772488, average train time (sec): 4.543000000012398e-05\n",
      "[189, 120] loss: 0.004051979741780088, validation loss: 0.004232680244933603, average train time (sec): 4.837250000093718e-05\n",
      "[189, 160] loss: 0.004538173408946022, validation loss: 0.004246339377648426, average train time (sec): 4.326499999933731e-05\n",
      "[190, 40] loss: 0.004017343145096675, validation loss: 0.004190544083819918, average train time (sec): 4.599499999926593e-05\n",
      "[190, 80] loss: 0.004160803620470688, validation loss: 0.004198696547767745, average train time (sec): 4.684249999797885e-05\n",
      "[190, 120] loss: 0.004261917027179152, validation loss: 0.00420881826694141, average train time (sec): 4.671000000087133e-05\n",
      "[190, 160] loss: 0.004342450440162793, validation loss: 0.004200990900467589, average train time (sec): 5.130499999950189e-05\n",
      "[191, 40] loss: 0.004001531121321023, validation loss: 0.004269274597425225, average train time (sec): 4.559749999941687e-05\n",
      "[191, 80] loss: 0.004064702725736424, validation loss: 0.004170169623322644, average train time (sec): 4.6567500001515326e-05\n",
      "[191, 120] loss: 0.004370256402762607, validation loss: 0.004196146797423936, average train time (sec): 4.705249999972239e-05\n",
      "[191, 160] loss: 0.004474602406844497, validation loss: 0.0041953702457249165, average train time (sec): 4.539749999992182e-05\n",
      "[192, 40] loss: 0.003908642038004473, validation loss: 0.0042135192889649915, average train time (sec): 5.1067500001522605e-05\n",
      "[192, 80] loss: 0.004220899171195924, validation loss: 0.004194149106108355, average train time (sec): 4.398250000008375e-05\n",
      "[192, 120] loss: 0.004223337984876707, validation loss: 0.004194997589297171, average train time (sec): 4.25499999977319e-05\n",
      "[192, 160] loss: 0.0043731510289944705, validation loss: 0.004132849774938429, average train time (sec): 4.7867500001075314e-05\n",
      "[193, 40] loss: 0.004133088077651337, validation loss: 0.004196631950589846, average train time (sec): 5.4587500000025105e-05\n",
      "[193, 80] loss: 0.004240929160732776, validation loss: 0.004181921442548903, average train time (sec): 4.822500000045693e-05\n",
      "[193, 120] loss: 0.004208476713392884, validation loss: 0.0041933459062832145, average train time (sec): 4.3644999999514765e-05\n",
      "[193, 160] loss: 0.004137647291645408, validation loss: 0.004208026431887498, average train time (sec): 4.352750000009564e-05\n",
      "[194, 40] loss: 0.004394419322488829, validation loss: 0.0042022834751614425, average train time (sec): 4.625750000002427e-05\n",
      "[194, 80] loss: 0.004372624738607556, validation loss: 0.004209725950616148, average train time (sec): 4.6924999998054774e-05\n",
      "[194, 120] loss: 0.0038994368689600377, validation loss: 0.0041886822338093, average train time (sec): 4.28799999980356e-05\n",
      "[194, 160] loss: 0.003938044619280845, validation loss: 0.004257932340480246, average train time (sec): 4.565750000153912e-05\n",
      "[195, 40] loss: 0.003959253092762083, validation loss: 0.004192034946754575, average train time (sec): 5.2395000000160505e-05\n",
      "[195, 80] loss: 0.004018909833393991, validation loss: 0.004208876830437836, average train time (sec): 4.365750000090429e-05\n",
      "[195, 120] loss: 0.004368692939169705, validation loss: 0.004245635754657241, average train time (sec): 4.478500000004715e-05\n",
      "[195, 160] loss: 0.004370447935070842, validation loss: 0.004239369544007306, average train time (sec): 4.2662499998868954e-05\n",
      "[196, 40] loss: 0.004084614233579487, validation loss: 0.004219643400437286, average train time (sec): 4.1182499998626554e-05\n",
      "[196, 80] loss: 0.004161182430107146, validation loss: 0.004243280699054869, average train time (sec): 5.2547500001765005e-05\n",
      "[196, 120] loss: 0.004283756332006306, validation loss: 0.00422497971405117, average train time (sec): 4.7815000002060515e-05\n",
      "[196, 160] loss: 0.004187271377304569, validation loss: 0.004188675024845409, average train time (sec): 4.940999999973883e-05\n",
      "[197, 40] loss: 0.003984398092143238, validation loss: 0.004237573769576145, average train time (sec): 4.668750000007549e-05\n",
      "[197, 80] loss: 0.004197454475797713, validation loss: 0.00420344393384063, average train time (sec): 4.734249999955864e-05\n",
      "[197, 120] loss: 0.004381644556997344, validation loss: 0.004231290839050176, average train time (sec): 4.425749999938944e-05\n",
      "[197, 160] loss: 0.004465330630773678, validation loss: 0.004215137312575331, average train time (sec): 4.848749999837309e-05\n",
      "[198, 40] loss: 0.004240234306780622, validation loss: 0.004193408454258768, average train time (sec): 5.047500000046057e-05\n",
      "[198, 80] loss: 0.004423451697221026, validation loss: 0.004265666249612311, average train time (sec): 4.7587500000645376e-05\n",
      "[198, 120] loss: 0.004068953718524426, validation loss: 0.004190594608069591, average train time (sec): 4.854499999851214e-05\n",
      "[198, 160] loss: 0.0039994366990868, validation loss: 0.0041900374575184205, average train time (sec): 4.942500000026939e-05\n",
      "[199, 40] loss: 0.004465080995578319, validation loss: 0.004166025646417489, average train time (sec): 4.854750000049535e-05\n",
      "[199, 80] loss: 0.00400435485644266, validation loss: 0.004192868445314608, average train time (sec): 5.446499999948173e-05\n",
      "[199, 120] loss: 0.0038125965395011006, validation loss: 0.004177569064645554, average train time (sec): 4.724499999895215e-05\n",
      "[199, 160] loss: 0.004248075949726626, validation loss: 0.00422837437165655, average train time (sec): 5.034499999965192e-05\n",
      "Saved checkpoint for epoch 199: PC_ConvMF_r12_cdim2-c00f\n",
      "Saved checkpoint for epoch 199: PC_ConvMF_r12_cdim2-c00f\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_run_dir):\n",
    "    os.mkdir(output_run_dir)\n",
    "\n",
    "# Save details\n",
    "with open(os.path.join(output_run_dir, f\"details_{runname}.json\"), \"w\" ) as write:\n",
    "    json.dump(run_details, write, indent=2 )\n",
    "\n",
    "\n",
    "for model in models:\n",
    "# for model in models[-1:]:\n",
    "    writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model.get_name()}_{runname}'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    train_model(model, optimizer, output_run_dir, **run_details[\"run_params\"], writer=writer, load=True)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b6ee6-a883-49ff-9f5f-24770bda2e8e",
   "metadata": {},
   "source": [
    "### Single model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df346cd8-400c-4862-a208-c0240bf79bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DANMF\n",
      "[0, 40] loss: 0.9970015108585357, validation loss: 0.9977373278365945, average train time (sec): 8.640750000381559e-05\n",
      "[0, 80] loss: 0.9970015019178391, validation loss: 0.9977372952227322, average train time (sec): 8.593250000217267e-05\n",
      "[0, 120] loss: 0.9960014522075653, validation loss: 0.9977373075935075, average train time (sec): 9.168500000100721e-05\n",
      "[0, 160] loss: 0.9970014408230782, validation loss: 0.9984920373502767, average train time (sec): 8.531999999945583e-05\n",
      "[1, 40] loss: 0.9960015803575516, validation loss: 0.9977373053442757, average train time (sec): 8.184499999970285e-05\n",
      "[1, 80] loss: 0.9970013990998268, validation loss: 0.9977373098427395, average train time (sec): 9.473500000467538e-05\n",
      "[1, 120] loss: 0.9970014497637749, validation loss: 0.9977373030950438, average train time (sec): 8.481999999503387e-05\n",
      "[1, 160] loss: 0.9980013832449913, validation loss: 0.9977372974719642, average train time (sec): 8.727500000986766e-05\n",
      "[2, 40] loss: 0.9970014646649361, validation loss: 0.9977372985965801, average train time (sec): 8.636249999653956e-05\n",
      "[2, 80] loss: 0.9990015625953674, validation loss: 0.9977373165904351, average train time (sec): 8.732500000405708e-05\n",
      "[2, 120] loss: 0.9930014714598656, validation loss: 0.9977373109673554, average train time (sec): 8.708750000323562e-05\n",
      "[2, 160] loss: 0.9980014637112617, validation loss: 0.9977373244627467, average train time (sec): 8.351500000571832e-05\n",
      "[3, 40] loss: 0.9960014000535011, validation loss: 0.9977372985965801, average train time (sec): 8.849250000366737e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_run_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA] if use_cuda else [profiler.ProfilerActivity.CPU],\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#                          record_shapes=False, \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#                          profile_memory=True, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#         with profiler.record_function(\"train_model\"):\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#             train_model(15, model, optimizer, checkpoint_at=5, writer=writer, load=False, profiler=prof)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, output_run_dir, epochs, checkpoint_at, load, batch_pr, writer, profiler, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m running_time \u001b[38;5;241m=\u001b[39m default_timer() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m---> 60\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Print and save statistics\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m batch_pr \u001b[38;5;241m==\u001b[39m batch_pr \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:    \u001b[38;5;66;03m# print every 200 mini-batches\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = Fork(nn_rank, img_size, [500, 200], [200, 300]).to(device)\n",
    "model = model_danmf\n",
    "runname = \"Better_opt\"\n",
    "output_run_dir = output_dir\n",
    "writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model.get_name()}_{runname}'))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "train_model(model, optimizer, output_run_dir, epochs=120, checkpoint_at=30, batch_pr=40, writer=writer, load=False)\n",
    "\n",
    "# with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA] if use_cuda else [profiler.ProfilerActivity.CPU],\n",
    "#                          record_shapes=False,\n",
    "#                          profile_memory=True,\n",
    "#                          # use_cuda=use_cuda,\n",
    "#                          schedule=torch.profiler.schedule(\n",
    "#                             wait=1,\n",
    "#                             warmup=1,\n",
    "#                             active=2,\n",
    "#                             repeat=1),\n",
    "#                          on_trace_ready=trace_handler\n",
    "#                          ) as prof:\n",
    "#         with profiler.record_function(\"train_model\"):\n",
    "#             train_model(15, model, optimizer, checkpoint_at=5, writer=writer, load=False, profiler=prof)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4507d-53f1-43b0-94d7-c0b0cdbdfcfd",
   "metadata": {},
   "source": [
    "#### Winners\n",
    "\n",
    "* Mac_Fork_sdim2-3ebc_fdim2-37e7_Conv_1\n",
    "* Mac_ConvMF_cdim2-c00f_Conv_1\n",
    "* Mac_ConvMF_cdim2-2d8c_Conv_1\n",
    "* Mac_ConvMF_cdim2-f2cd_Conv_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ead22-e9f7-4ddf-b784-f218099a0b77",
   "metadata": {},
   "source": [
    "### Setup to load and run against SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7106ac4-6e66-4dce-9f86-234a5b9a2c9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Params\n",
    "runname = \"Conv_1\"\n",
    "model_str_name = \"ConvMF_cdim2-f2cd\"\n",
    "\n",
    "model = load_trained_model(ConvMF, \"Mac\", runname, model_str_name, output_run_dir)\n",
    "print(model, model.get_name())\n",
    "\n",
    "# writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model.get_name()}_{runname}'))\n",
    "\n",
    "# def save_prof(prof):\n",
    "#     prof_events = prof.key_averages()\n",
    "\n",
    "# with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA] if use_cuda else [profiler.ProfilerActivity.CPU],\n",
    "#                          record_shapes=False,\n",
    "#                          profile_memory=True,\n",
    "#                       on_trace_ready=save_prof\n",
    "#                          ) as prof:\n",
    "#     U, V = model(next(iter(validation_dataloader)))\n",
    "#     prof.step()\n",
    "#     prof_events = prof.key_averages()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2c681-0156-487e-916c-fa38f509e40f",
   "metadata": {},
   "source": [
    "## Comparision against base\n",
    "\n",
    "Compare against some python implementation of SVD that produces U: m x r V: r x n matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c35fe-fae7-4099-a20b-4218ea73c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model.get_name()}_{runname}'))\n",
    "# writer = SummaryWriter(os.path.join(tensorboard_dir, f'Mac_{model.get_name()}_{runname}_Winner'))\n",
    "\n",
    "## Params\n",
    "svd_rank = 5\n",
    "step = 2 # For tensorboard, to set what \"global step\" the histograms occured at\n",
    "\n",
    "losses_nn = []\n",
    "losses_svd = []\n",
    "times_nn = []\n",
    "times_svd = []\n",
    "\n",
    "\n",
    "# imgs = next(iter(validation_dataloader))\n",
    "\n",
    "for img in test_dataloader:\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Run and time the NN\n",
    "    start_nn = default_timer()\n",
    "\n",
    "    model.eval()\n",
    "    U_nn, V_nn = model(img)\n",
    "    U_nn = torch.squeeze(U_nn).cpu().detach().numpy()\n",
    "    V_nn = torch.squeeze(V_nn).cpu().detach().numpy()\n",
    "    UV_nn = np.maximum(np.matmul(U_nn, V_nn), 0)\n",
    "\n",
    "    end_nn = default_timer() - start_nn\n",
    "    times_nn.append(end_nn)\n",
    "\n",
    "    img = torch.squeeze(img.to(\"cpu\"))\n",
    "\n",
    "    # Run and time standard SVD, truncating to rank\n",
    "    svds = []\n",
    "    start_svd = default_timer()\n",
    "\n",
    "    U_svd, S, V_svd = svd(img)\n",
    "    S = S[:svd_rank]\n",
    "    U_svd = U_svd[:, :svd_rank]\n",
    "    V_svd = V_svd[:svd_rank, :]\n",
    "    svd_USV = U_svd.dot(diagsvd(S, svd_rank, svd_rank)).dot(V_svd)\n",
    "\n",
    "    # for i in range(batch_size):\n",
    "    #     U_svd, S, V_svd = svd(imgs[i, :, :])\n",
    "    #     S = S[:svd_rank]\n",
    "    #     U_svd = U_svd[:, :svd_rank]\n",
    "    #     V_svd = V_svd[:svd_rank, :]\n",
    "    #     svds.append(U_svd.dot(diagsvd(S, svd_rank, svd_rank)).dot(V_svd))\n",
    "\n",
    "    end_svd = default_timer() - start_svd\n",
    "    times_svd.append(end_svd)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_nn = np.square(np.linalg.norm(img - UV_nn, ord=\"fro\"))\n",
    "    loss_svd = np.square(np.linalg.norm(img - svd_USV, ord=\"fro\"))\n",
    "    losses_nn.append(loss_nn)\n",
    "    losses_svd.append(loss_svd)\n",
    "\n",
    "writer.add_histogram(\"Model avg loss\", np.array(losses_nn), step)\n",
    "writer.add_histogram(\"SVD avg loss\", np.array(losses_svd), step)\n",
    "writer.add_histogram(\"Model time\", np.array(times_nn), step)\n",
    "writer.add_histogram(\"SVD time\", np.array(times_svd), step)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "print(\"SimpleNN:\\n\", \"Loss (avg) =\", np.mean(losses_nn), \"\\tTime =\", np.mean(times_nn))\n",
    "print(\"Scipy SVD:\\n\", \"Loss (avg) =\", np.mean(losses_svd), \"\\tTime =\", np.mean(times_svd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660ae80-8521-4adb-8555-e0634ef57f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_svd, S, V_svd = svd(imgs[i, :, :])\n",
    "# plt.semilogy(S)\n",
    "plt.semilogy(np.sqrt(np.linalg.norm(S)**2 - np.cumsum(S**2)) / np.linalg.norm(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648029f-e5aa-4ca5-99f4-b1ad04078edb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* ~~Save hyperparameters, run params to dict (json?)~~\n",
    "* ~~Run against bigger networks~~\n",
    "* ~~Compare rank vs err (loss) for SVD vs NN, and see how many more ranks it is to get comparable err~~\n",
    "    * ~~compare against SVD 4, using rank 6 for NN (or bigger)~~\n",
    "    * <img src=\"images/rank_vs_err.png\" alt=\"Rank vs Error Plot\" style=\"width: 600px;\"/>\n",
    "* ~~Use a different algo where we pre-give the rank, better alg: `scipy.sparse.linalg.svds`, tol=0.001~~\n",
    "* Impose nonneg on U, V BEFORE multiplying\n",
    "* ~~Generate the data~~\n",
    "    * ~~Make a movie of hte fs, to sanity check~~\n",
    "* Try supervised learning with `torch.nn.KLDivLoss(reduction = \"batchmean\")`\n",
    "    * Remember needs log input from nn!\n",
    "    * Figure out S? \n",
    "* Research how big 1st layer should be in comparison to dims of  input\n",
    "* Get access to NERSC?\n",
    "* ~~Go as big for imgs as I can, log space increase in size~~\n",
    "* Try running at bigger network and input sizes, (dont even train) and just test timings\n",
    "\n",
    "\n",
    "## Model: Variations\n",
    "* ~~Simple with no nonnegativity constraints (sigmoid activation)~~\n",
    "    * ~~Allow negativity at every step except after UV, aka ReLU(UV) or |UV| (relu better)~~\n",
    "* ~~Different activation layers~~\n",
    "* ~~Different structure~~\n",
    "    * ~~Try forking at beginning~~\n",
    "* Define new layer\n",
    "    * Instead of flattening and using a linear layer, doing a either side multiply of the matrix input: A\\*X\\*B where A: n_1 x m, X: m x n, B: n x n_2\n",
    "* ~~try 2d convolutional layers~~\n",
    "\n",
    "### Structure\n",
    "* Try dropout\n",
    "* Try regularization\n",
    "* Try different measured weight matrix sizes\n",
    "\n",
    "## Optiona\n",
    "* Randomized Numerical Linear Algebra: Foundations & Algorithms (Per-Gunnar Martinsson, University of Texas at Austin Joel A. Tropp, California Institute of Technology)l\n",
    "*  Pad input when not enough imgs for batch\n",
    "\n",
    "### Data Augmentation\n",
    "*  Cropping, rotating, scaling, reflecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4b44f-e13c-4e04-a7a7-c25afd9bef5d",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74a448-6846-44cc-bdf8-f3964cdd7a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"../data/output/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199b44d-598a-4a3f-be32-9e82ed3d3e55",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. <a id='r1'></a> De Handschutter, P., Gillis, N., & Siebert, X. (2021). A survey on deep matrix factorizations. Computer Science Review, 42, 100423. https://doi.org/10.1016/j.cosrev.2021.100423\n",
    "2. <a id='r2'></a> Sun, J., Kong, Q., & Xu, Z. (2022). Deep alternating non-negative matrix factorisation. Knowledge-Based Systems, 251, 109210. https://doi.org/10.1016/j.knosys.2022.109210\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64465e29-7aa1-4973-8eea-11ada04d614e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
